########################################################################################################################################
# Function used in the script
########################################################################################################################################
# STRATEGY: Modular helper functions for data processing and statistical analysis
# - Statistical testing functions for normality and multiple comparisons
# - Data processing pipeline for FluorCam files
# - Visualization functions for bar plots and curve analysis
# - Separation of concerns: each function has a single responsibility
# - Reusable components that can be tested independently


# ===========================================
# SECTION 1: DATA PROCESSING FUNCTIONS
# ===========================================
# PURPOSE: Handle FluorCam file processing and data preparation
# STRATEGY: Modular pipeline for file reading, cleaning, and transformation

# Function to process data files
#========================================================================================================================================
# STRATEGY: Complete data processing pipeline for FluorCam files
# PURPOSE: Transform raw .TXT files into analysis-ready dataframe
# WORKFLOW: File discovery → cleaning → calculation → naming → merging
# INPUT: File pattern, directory path, and variable naming scheme
# OUTPUT: Combined dataframe ready for statistical analysis

process_data_files <- function(pattern, var1, var2, var3, dirpath) {
  # FILE DISCOVERY
  # STRATEGY: Use pattern matching to find relevant files
  # PURPOSE: Flexible file selection based on user input
  files <- list.files(path = dirpath, pattern = pattern, full.names = TRUE)
  
  # INNER FUNCTION: FILE CLEANING
  # STRATEGY: Nested function for single responsibility
  # PURPOSE: Remove FluorCam header lines and read data
  remove_first_two_lines <- function(file_name, area) {
    # READ ALL LINES
    # STRATEGY: Read entire file first for flexible processing
    lines <- readLines(file_name)

    # REMOVE EMPTY LINES
    # STRATEGY: Clean data by removing blank lines
    # PURPOSE: Prevent parsing errors from empty rows
    lines <- lines[lines != ""]

    # REMOVE HEADER LINES
    # STRATEGY: FluorCam files have 2-line headers that must be removed
    # PURPOSE: Leave only the data table for proper parsing
    if(length(lines) > 2){
      lines <- lines[-c(1,2)]  # Remove first two lines
    } else {
      # ERROR HANDLING
      # STRATEGY: Informative error message for insufficient data
      stop("Le fichier ne contient pas assez de lignes.")
    }

    # PARSE DATA TABLE
    # STRATEGY: Use read.table with tab separation (FluorCam standard)
    # PURPOSE: Convert cleaned text to structured dataframe
    data <- read.table(text = lines, sep = "\t", header = TRUE)
    return(data)
  }

  # INNER FUNCTION: Fv/Fm CALCULATION
  # STRATEGY: Automatic calculation of key fluorescence parameter
  # PURPOSE: Fv/Fm is standard measure of photosynthetic efficiency
  # FORMULA: Fv/Fm = (Fm - F0) / Fm = Fv / Fm
  compute_Fv_Fm <- function(df) {
    df$Fv_Fm <- df$Fv / df$Fm
    return(df)
  }

  # INNER FUNCTION: NAME COLUMN ADDITION
  # STRATEGY: Add filename as identifier column
  # PURPOSE: Track data source for later variable extraction
  add_name_column <- function(df, name) {
    df$Name <- name
    return(df)
  }

  # INNER FUNCTION: VARIABLE EXTRACTION
  # STRATEGY: Parse filename into separate variable columns
  # PURPOSE: Extract experimental variables from systematic naming
  # METHOD: Split on underscore separator (VAR1_VAR2_VAR3.TXT)
  divide_name <- function(df) {
    df <- tidyr::separate(
      data = df, 
      col = "Name", 
      into = c(var1, var2, var3),  # User-defined variable names
      sep = "_",                   # Underscore separator
      remove = TRUE                # Remove original Name column
    )
    return(df)
  }

  # MAIN PROCESSING PIPELINE
  # STRATEGY: Apply processing functions to all files

  # STEP 1: CLEAN ALL FILES
  # STRATEGY: lapply for efficient list processing
  # PURPOSE: Apply cleaning function to each file
  Liste <- lapply(files, remove_first_two_lines, area = "")
  # CREATE NAMED LIST
  # STRATEGY: Use filenames (without extension) as list names
  # PURPOSE: Maintain file identity through processing
  names(Liste) <- tools::file_path_sans_ext(basename(files))

  # STEP 2: TRANSPOSE DATA
  # STRATEGY: FluorCam data comes with parameters as rows, need columns
  # PURPOSE: Transform from parameter-per-row to parameter-per-column
  # METHOD: data.table::transpose with X column as names
  Liste <- lapply(Liste, data.table::transpose, make.names = "X")

  # STEP 3: CALCULATE Fv/Fm
  # STRATEGY: Apply calculation to all datasets
  # PURPOSE: Add derived parameter to all files
  Liste <- lapply(Liste, compute_Fv_Fm)

  # STEP 4: ADD FILENAME IDENTIFIERS
  # STRATEGY: Use names() to apply filename to each dataset
  # PURPOSE: Prepare for variable extraction
  Liste <- lapply(names(Liste), function(name) {
    add_name_column(Liste[[name]], name)
  })

  # STEP 5: EXTRACT VARIABLES FROM FILENAMES
  # STRATEGY: Parse systematic filenames into experimental variables
  # PURPOSE: Create grouping variables for statistical analysis
  Liste <- lapply(Liste, divide_name)

  # STEP 6: COMBINE ALL DATA
  # STRATEGY: Row-bind all processed datasets
  # PURPOSE: Create single analysis-ready dataframe
  df <- do.call(rbind, Liste)


  return(df)
}

# ===========================================
# SECTION 2: STATISTICAL TESTING FUNCTIONS
# ===========================================
# PURPOSE: Provide robust statistical analysis capabilities
# STRATEGY: Separate functions for different statistical procedures

# Define a function to check normality status of the data
#========================================================================================================================================
# STRATEGY: Global normality assessment across all data groups
# PURPOSE: Determine whether to use parametric or non-parametric tests
# LOGIC: If ANY group fails normality test, use non-parametric methods for ALL groups
# WHY: Ensures consistent statistical approach across entire analysis

check_normality <- function(shapiro_df) {
  # STRATEGY: Conservative approach to normality testing
  # PURPOSE: Assume normality unless proven otherwise
  flag_normal <- TRUE

  # LOOP THROUGH ALL GROUP RESULTS
  # STRATEGY: Break on first non-normal group for efficiency
  # PURPOSE: Single failure invalidates parametric assumptions
  for (i in seq_len(nrow(shapiro_df))) {
    if (shapiro_df$p[i] <= 0.05) {
      # CRITICAL DECISION POINT
      # STRATEGY: Strict alpha = 0.05 threshold for normality
      # PURPOSE: Conservative approach ensures valid statistical inference
      flag_normal <- FALSE
      break  # Exit immediately - no need to check remaining groups
    }
  }
  
  return(flag_normal)
}


# ===========================================
# SECTION 3: VISUALIZATION FUNCTIONS
# ===========================================
# PURPOSE: Generate publication-quality plots with statistical annotations
# STRATEGY: Separate functions for different plot types with comprehensive options

# Function to plot Bar plot
#========================================================================================================================================
# STRATEGY: Comprehensive bar plot with automatic statistical testing
# PURPOSE: Publication-ready bar plots with significance testing and annotations
# FEATURES: Automatic normality testing, appropriate statistical tests, CLD annotations
# INPUT: Data, variables, measure column, ordering, and color options
# OUTPUT: ggplot object with statistical annotations and summary statistics

analyse_barplot <- function(
  data,
  var1, var2, measure_col, 
  var1_order = NULL, var2_order = NULL,
  fill_color = "ivory1", line_color = "darkgrey", point_color = "darkgreen"
) {

  # ===========================================
  # INPUT VALIDATION SECTION
  # ===========================================
  # STRATEGY: Comprehensive input checking before processing
  # PURPOSE: Prevent errors and provide clear feedback

  if(is.null(data) || nrow(data) == 0) {
    stop("Data is empty or NULL")
  }

  if(!var1 %in% colnames(data)) {
    stop(paste("Variable", var1, "not found in data"))
  }

  if(!var2 %in% colnames(data)) {
    stop(paste("Variable", var2, "not found in data"))
  }

  if(!measure_col %in% colnames(data)) {
    stop(paste("Measure column", measure_col, "not found in data"))
  }

  # ===========================================
  # DATA PREPARATION SECTION
  # ===========================================
  # STRATEGY: Convert to factors and apply user-specified ordering
  # PURPOSE: Control plot appearance and ensure consistent grouping

  # FACTOR CONVERSION WITH ORDERING
  # STRATEGY: Apply user-specified order if provided, otherwise use default
  # PURPOSE: User control over plot layout and legend order
  if(!is.null(var1_order)) {
    data[[var1]] <- factor(data[[var1]], levels = var1_order)
  } else {
    data[[var1]] <- as.factor(data[[var1]])
  }

  if(!is.null(var2_order)) {
    data[[var2]] <- factor(data[[var2]], levels = var2_order)
  } else {
    data[[var2]] <- as.factor(data[[var2]])
  }

  # ===========================================
  # PARAMETRIC ANALYSIS BRANCH
  # ===========================================
  # STRATEGY: Full parametric pipeline with ANOVA and Tukey HSD
  # PURPOSE: When normality assumptions are met

  if (flag_normal) {
    # SUMMARY STATISTICS
    # STRATEGY: Use summarise for mean ± standard error
    # PURPOSE: Generate values for bar heights and error bars
    my_summary <- data %>%
      group_by(.data[[var2]], .data[[var1]]) %>%
      summarise(
        N = n(),
        mean_value = mean(.data[[measure_col]], na.rm = TRUE),
        sd_value = sd(.data[[measure_col]], na.rm = TRUE),
        se = sd_value / sqrt(N),
        ci_lower = mean_value - se * qt(0.975, df = N - 1),
        ci_upper = mean_value + se * qt(0.975, df = N - 1),
        .groups = 'drop'
      ) 

    # ANOVA TESTING - FORMULA APPROACH
    # STRATEGY: Use formula approach which is more reliable with rstatix
    # PURPOSE: Test for overall differences before post-hoc testing

    # Create a temporary column with a fixed name for rstatix
    data_temp <- data
    data_temp$temp_measure <- data_temp[[measure_col]]

    anova_result <- data_temp %>%
      group_by(.data[[var1]]) %>%
      rstatix::anova_test(temp_measure ~ .data[[var2]]) %>%
      select(-term)  # Remove the 'term' column that rstatix adds

    # TUKEY POST-HOC TESTING - FORMULA APPROACH
    # STRATEGY: Use formula approach for rstatix compatibility
    # PURPOSE: Identify which specific groups differ

    tukey_results <- data_temp %>%
      group_by(.data[[var1]]) %>%
      rstatix::tukey_hsd(temp_measure ~ .data[[var2]])

    # COMPACT LETTER DISPLAY
    # STRATEGY: Convert p-values to letter annotations
    # PURPOSE: Visual indication of statistical groupings
    cld_table_parametric <- generate_cld_parametric(tukey_results, var1, var2)

    # MERGE SUMMARY WITH CLD
    # STRATEGY: Combine statistical results with summary data
    # PURPOSE: Single dataframe for plotting with all needed information
    df2 <- merge(my_summary, cld_table_parametric, by = c(var2, var1), all.x = TRUE)

    # PRESERVE FACTOR ORDERING
    # STRATEGY: Ensure user-specified order is maintained after merge
    # PURPOSE: Plot appears as user intended
    if(!is.null(var1_order)) {
      df2[[var1]] <- factor(df2[[var1]], levels = var1_order)
    }
    if(!is.null(var2_order)) {
      df2[[var2]] <- factor(df2[[var2]], levels = var2_order)
    }

    # PLOT CONSTRUCTION (PARAMETRIC)
    # STRATEGY: Layered ggplot with statistical annotations
    # PURPOSE: Professional publication-quality visualization
    p <- df2 %>%
      ggplot(aes(x = .data[[var2]], y = .data[[measure_col]], 
                 fill = .data[[var2]])) +

      # BAR LAYER
      # STRATEGY: geom_col for exact heights (not count-based)
      # PURPOSE: Show mean values with custom styling
      geom_col(color = line_color, width = 0.6, position = position_dodge2(padding = 0.05)) +

      # FILL COLOR SCALE
      # STRATEGY: Manual color specification for consistency
      # PURPOSE: User control over plot appearance
      scale_fill_manual(values = rep(fill_color, length(unique(df2[[var2]])))) +

      # Y-AXIS SCALING
      # STRATEGY: Start at zero with small expansion for CLD labels
      # PURPOSE: Honest representation with space for annotations
      scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +

      # DATA POINTS OVERLAY
      # STRATEGY: Show individual observations as points
      # PURPOSE: Transparency about data distribution and sample size
      geom_quasirandom(
        data = data,  # Use original data, not summary
        aes(x = .data[[var2]], y = .data[[measure_col]]), 
        color = point_color, width = 0.3, alpha = 0.6
      ) +

      # ERROR BARS
      # STRATEGY: Standard error bars from summary statistics
      # PURPOSE: Show uncertainty in mean estimates
      geom_segment(aes(x = .data[[var2]], xend = .data[[var2]], 
                       y = pmax(0, .data[[measure_col]] - ci), 
                       yend = .data[[measure_col]] + ci), 
                   color = "black") +

      # SIGNIFICANCE LETTERS
      # STRATEGY: Text annotations above bars
      # PURPOSE: Clear indication of statistical groupings
      geom_text(aes(x = .data[[var2]], 
                    y = .data[[measure_col]] + (0.15 * max(df2[[measure_col]], na.rm = TRUE)), 
                    label = cld), 
                size = 3, inherit.aes = TRUE) +

      # THEME AND STYLING
      # STRATEGY: Clean, professional theme
      # PURPOSE: Publication-ready appearance
      theme_classic() +
      theme(
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.line.x = element_line(linewidth = 0.5),
        axis.line.y = element_line(linewidth = 0.5),
        panel.background = element_rect(fill = 'transparent', color = NA),
        plot.background = element_rect(fill = 'transparent', color = NA),
        axis.text.y = element_text(vjust = 1),
        legend.position = "none",  # Remove legend (redundant with x-axis)
        strip.background = element_blank(),
        strip.placement = "outside",
        strip.text = element_text(face = "plain", size = 10, color = "black", hjust = 0.5)
      ) +

      # FACETING
      # STRATEGY: Panel separation by facet variable
      # PURPOSE: Clear separation of different experimental conditions
      facet_wrap(as.formula(paste("~", var1)), nrow = 1, scales = "free_y") +

      # AXIS LABELS
      # STRATEGY: Use variable names for clarity
      # PURPOSE: Self-documenting plots
      labs(x = var1, y = measure_col)

    # RETURN PARAMETRIC RESULTS
    # STRATEGY: Return both plot and all statistical results
    # PURPOSE: Enable export of complete analysis
    return(list(
      plot = p,
      summary = my_summary,
      shapiro = shapiro_df,
      anova = anova_result,
      tukey = tukey_results,
      cld = cld_table_parametric
    ))

  } else {
    # ===========================================
    # NON-PARAMETRIC ANALYSIS BRANCH
    # ===========================================
    # STRATEGY: Non-parametric pipeline with Kruskal-Wallis and Dunn tests
    # PURPOSE: When normality assumptions are violated

    # MEDIAN WITH CONFIDENCE INTERVALS - REPLACE groupwiseMedian
    # STRATEGY: Use modern dplyr with quantile-based confidence intervals
    # PURPOSE: Non-parametric equivalent of mean ± SE
    conf_int <- data %>%
      group_by(.data[[var2]], .data[[var1]]) %>%
      summarise(
        N = n(),
        Median = median(.data[[measure_col]], na.rm = TRUE),
        Q1 = quantile(.data[[measure_col]], 0.25, na.rm = TRUE),
        Q3 = quantile(.data[[measure_col]], 0.75, na.rm = TRUE),
        Percentile.lower = quantile(.data[[measure_col]], 0.025, na.rm = TRUE),  # 2.5th percentile
        Percentile.upper = quantile(.data[[measure_col]], 0.975, na.rm = TRUE),  # 97.5th percentile
        .groups = 'drop'
      )

    # KRUSKAL-WALLIS TESTING - FORMULA APPROACH
    # STRATEGY: Non-parametric equivalent of ANOVA
    # PURPOSE: Test for overall differences between groups

    # Use the same temporary column approach
    data_temp <- data
    data_temp$temp_measure <- data_temp[[measure_col]]

    kruskal_pval <- data_temp %>%
      group_by(.data[[var1]]) %>%
      rstatix::kruskal_test(temp_measure ~ .data[[var2]]) %>%
      dplyr::select(.data[[var1]], p)

    # SIGNIFICANCE CHECK
    # STRATEGY: Only proceed with post-hoc if overall test is significant
    # PURPOSE: Prevent multiple comparisons when not justified
    significant <- any(kruskal_pval$p < 0.05)

    if (significant) {
      # DUNN POST-HOC TESTING
      # STRATEGY: Non-parametric pairwise comparison
      # PURPOSE: Identify which specific groups differ
      pval_dunn <- test_dunn(data, var1, var2, measure_col)

      # COMPACT LETTER DISPLAY (NON-PARAMETRIC)
      # STRATEGY: Generate letters from Dunn test results
      # PURPOSE: Visual grouping for non-parametric results
      cld_table_nonparametric <- generate_cld_nonparametric(pval_dunn, var1, var2)

      # MERGE AND PREPARE PLOTTING DATA
      df2 <- merge(conf_int, cld_table_nonparametric, by.x = c(var2, var1), by.y = c(var2, var1))
      df2[[var1]] <- factor(df2[[var1]], levels = var1_order)
      df2[[var2]] <- factor(df2[[var2]], levels = var2_order)

      # PLOT CONSTRUCTION (NON-PARAMETRIC)
      # STRATEGY: Similar to parametric but using medians and percentile CIs
      # PURPOSE: Appropriate visualization for non-parametric analysis
      p <- df2 %>%
        ggplot(aes(x = .data[[var2]], y = Median, fill = .data[[var2]])) +

        # MEDIAN BARS
        # STRATEGY: Show medians instead of means
        # PURPOSE: Appropriate central tendency for non-normal data
        geom_col(color = line_color, width = 0.6, position = position_dodge2(padding = 0.05)) +
        scale_fill_manual(values = rep(fill_color, length(unique(df2[[var2]])))) +
        scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +

        # DATA POINTS OVERLAY
        # STRATEGY: Same as parametric version
        # PURPOSE: Show actual data distribution
        geom_quasirandom(
          data = data, 
          aes(x = .data[[var2]], y = .data[[measure_col]]), 
          color = point_color, width = 0.3, alpha = 0.6
        ) +

        # CONFIDENCE INTERVAL BARS
        # STRATEGY: Use bootstrap percentile confidence intervals
        # PURPOSE: Show uncertainty in median estimates
        geom_segment(aes(x = .data[[var2]], xend = .data[[var2]], 
                         y = pmax(0, Percentile.lower), yend = Percentile.upper), 
                     color = "black") +

        # SIGNIFICANCE LETTERS
        # STRATEGY: Position relative to confidence intervals
        # PURPOSE: Clear statistical grouping indication
        geom_text(aes(x = .data[[var2]], y = Percentile.upper + (0.15 * Median), label = cld), 
                  size = 3, inherit.aes = TRUE) +

        # THEME (SAME AS PARAMETRIC)
        theme_classic() +
        theme(
          axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
          axis.line.x = element_line(linewidth = 0.5),
          axis.line.y = element_line(linewidth = 0.5),
          panel.background = element_rect(fill = 'transparent', color = NA),
          plot.background = element_rect(fill = 'transparent', color = NA),
          axis.text.y = element_text(vjust = 1),
          legend.position = "none",
          strip.background = element_blank(),
          strip.placement = "outside",
          strip.text = element_text(face = "plain", size = 10, color = "black", hjust = 0.5)
        ) +
        facet_wrap(as.formula(paste("~", var1)), nrow = 1, scales = "free_y") +
        labs(x = var1, y = measure_col)

      # RETURN NON-PARAMETRIC RESULTS
      # STRATEGY: Return all relevant non-parametric statistics
      # PURPOSE: Complete analysis package for export
      return(list(
        plot = p,
        summary = conf_int,
        shapiro = shapiro_df,
        kruskal = kruskal_pval,
        dunn = pval_dunn,
        cld = cld_table_nonparametric
      ))
    } else {
      # NO SIGNIFICANT DIFFERENCES
      # STRATEGY: Return informative message instead of plot
      # PURPOSE: Avoid misleading post-hoc testing when overall test is non-significant
      return("Data are not significantly different, the Dunn test was not performed.")
    }
  }
}




